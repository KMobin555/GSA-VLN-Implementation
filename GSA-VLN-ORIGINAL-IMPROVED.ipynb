{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"cell-md-0","cell_type":"markdown","source":"# GSA-VLN: Full Implementation with Cross-Episode Parameter Updates\n## [General Scene Adaptation for Vision-and-Language Navigation](https://arxiv.org/pdf/2501.17403)\n\n### What's New vs. Original Notebook:\n| Component | Before | After |\n|---|---|---|\n| GraphMap persistence | Episodes in same scene | ✅ True environment-level persistence |\n| Cross-episode parameter updates | ❌ Missing | ✅ Model weights update after each episode using memory bank |\n| Unsupervised adaptation loop | ❌ ~10% | ✅ Full Eq.3 from paper: θ' = θ - α∇L(M_E, θ) |\n| Backprop bug (line 728) | ❌ loss.item() broke gradient | ✅ Fixed: tensor loss flows to optimizer |\n| Fine-tuning loop | ❌ Random scene sampling | ✅ Sequential per-scene instruction execution |\n| Memory bank | ❌ Not implemented | ✅ Stores O, X, A, P across all episodes per scene |\n\n### Paper Equations Implemented:\n- **Eq.1**: `M_E = {X_1:k, O_1:k, A_1:k, P_1:k}` → `MemoryBank` class\n- **Eq.2**: `a_0 = π(O_0, X, H_0; θ)` where `H_0 = M'_E ⊆ M_E` → GraphMap loaded from memory\n- **Eq.3**: `θ' = θ - α∇L(M_E, θ)` → `unsupervised_adaptation_step()` function\n- **Eq.4**: `max_{θ_0} E[P(E; θ'(θ_0))]` → pretraining on general data before scene-specific adaptation","metadata":{}},{"id":"cell-md-1","cell_type":"markdown","source":"## Section 1: Installation & Environment Setup","metadata":{}},{"id":"cell-code-1","cell_type":"code","source":"!pip install torch torchvision torchaudio transformers -q\n!pip install numpy pandas matplotlib seaborn networkx tqdm scipy -q\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport json\nimport networkx as nx\nfrom collections import defaultdict, deque\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Optional\nimport random\nimport copy\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport pickle\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB\")\nelse:\n    print(\"  Running on CPU — all training will still work, just slower\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-2","cell_type":"markdown","source":"## Section 2: Dataset — NavigationGraph, GraphMap, MemoryBank\n\n**What changed vs. original:**\n- `GraphMap.__init__` now takes `scene` object to use a stable fixed start node (not instruction-dependent)\n- Added `MemoryBank` class — this is **Eq.1** from the paper: stores O, X, A, P across ALL episodes in a scene\n- `R2RLikeDataset` groups instructions by scene for sequential execution","metadata":{}},{"id":"cell-code-2","cell_type":"code","source":"class NavigationGraph:\n    \"\"\"Simulated Matterport3D scene connectivity graph.\n    In the real paper: loaded from MP3D/HM3D scan JSON files.\n    Here: randomly generated connected graph with same structure.\n    \"\"\"\n    def __init__(self, graph_id: str, num_nodes: int = 20):\n        self.graph_id = graph_id\n        self.nodes = set()\n        self.edges = defaultdict(set)\n        self.node_positions = {}\n        self.node_features = {}\n        self.graph = nx.Graph()\n        self._generate_random_scene(num_nodes)\n\n    def _generate_random_scene(self, num_nodes: int):\n        for i in range(num_nodes):\n            vp = f\"vp_{i}\"\n            self.nodes.add(vp)\n            self.graph.add_node(vp)\n            self.node_positions[vp] = {\n                'x': np.random.uniform(-10, 10),\n                'y': np.random.uniform(-10, 10),\n                'z': np.random.uniform(0, 5),\n            }\n            # 256-dim placeholder for ViT-B/16 CLIP features (paper uses 2048-dim)\n            self.node_features[vp] = np.random.randn(256).astype(np.float32)\n\n        nodes_list = list(self.nodes)\n        for i in range(len(nodes_list) - 1):\n            self.add_edge(nodes_list[i], nodes_list[i + 1])\n            if np.random.rand() < 0.5:\n                self.add_edge(nodes_list[i], np.random.choice(nodes_list))\n\n    def add_edge(self, from_vp: str, to_vp: str):\n        if from_vp in self.nodes and to_vp in self.nodes:\n            self.edges[from_vp].add(to_vp)\n            self.edges[to_vp].add(from_vp)\n            self.graph.add_edge(from_vp, to_vp)\n\n    def get_neighbors(self, vp: str) -> List[str]:\n        return list(self.edges.get(vp, []))\n\n    def get_feature(self, vp: str) -> np.ndarray:\n        return self.node_features.get(vp, np.zeros(256, dtype=np.float32))\n\n    def get_fixed_start(self) -> str:\n        \"\"\"Returns a stable reference node for this scene (not instruction-dependent).\n        CHANGE from original: was using trajectory[0]['viewpoint'] which varied per episode.\n        Now always the same node, so GraphMap starts from consistent anchor.\n        \"\"\"\n        return sorted(list(self.nodes))[0]  # deterministic: always 'vp_0'\n\n\n# =============================================================================\n# GraphMap — Paper Section 3.2, part of M_E\n# CHANGE: Now initialized with scene's fixed_start, not trajectory[0]\n# CHANGE: Survives across ALL episodes in the same environment\n# =============================================================================\nclass GraphMap:\n    \"\"\"Persistent topological memory of explored locations in ONE environment.\n\n    This implements GR-DUET's 'Graph-Retained' mechanism:\n    - Traditional DUET: graph resets to empty at start of each instruction\n    - GR-DUET / GSA-VLN: graph PERSISTS and GROWS across all instructions in the scene\n\n    Paper Eq.1: M_E stores O (observations) per viewpoint.\n    This class handles the graph/spatial part of M_E.\n    MemoryBank (below) handles the full M_E including instructions and actions.\n    \"\"\"\n    def __init__(self, fixed_start_vp: str):\n        # CHANGE: fixed_start_vp comes from scene.get_fixed_start(), not from trajectory\n        self.start_vp = fixed_start_vp\n        self.node_positions = {fixed_start_vp: {'x': 0, 'y': 0, 'z': 0}}\n        self.node_embeds = {fixed_start_vp: np.zeros(256, dtype=np.float32)}\n        self.graph = nx.Graph()\n        self.graph.add_node(fixed_start_vp)\n        self.node_visit_order = [fixed_start_vp]\n        self.node_step_ids = {fixed_start_vp: 0}\n        self.global_step = 0  # CHANGE: counts steps across ALL episodes, not just current one\n        self.episode_count = 0  # CHANGE: tracks how many episodes have used this map\n\n    def update_graph(self, vp: str, position: Dict, embed: np.ndarray, neighbors: List[str]):\n        \"\"\"Called at every navigation step. Graph grows persistently.\"\"\"\n        if vp not in self.graph:\n            self.node_positions[vp] = position\n            self.node_embeds[vp] = embed\n            self.node_step_ids[vp] = self.global_step\n            self.node_visit_order.append(vp)\n            self.graph.add_node(vp)\n        else:\n            # Running average to refine embedding over repeated visits\n            # Guard: node may be in graph but not yet in node_embeds (e.g. added as neighbor)\n            if vp in self.node_embeds:\n                self.node_embeds[vp] = 0.9 * self.node_embeds[vp] + 0.1 * embed\n            else:\n                self.node_embeds[vp] = embed\n                self.node_positions[vp] = position\n                if vp not in self.node_visit_order:\n                    self.node_visit_order.append(vp)\n\n        for neighbor in neighbors:\n            self.graph.add_edge(vp, neighbor)\n\n        self.global_step += 1\n\n    def mark_episode_end(self):\n        \"\"\"CHANGE: Called at end of each episode. Graph is NOT reset — it persists.\"\"\"\n        self.episode_count += 1\n        # In the real GR-DUET, node embeddings can be refined here\n        # Our simplified version just increments the counter\n\n    def get_all_visited_nodes(self) -> List[str]:\n        return self.node_visit_order\n\n    def get_node_embed(self, vp: str) -> np.ndarray:\n        return self.node_embeds.get(vp, np.zeros(256, dtype=np.float32))\n\n    def node_count(self) -> int:\n        return len(self.node_positions)\n\n\n# =============================================================================\n# MemoryBank — Paper Eq.1: M_E = {X_1:k, O_1:k, A_1:k, P_1:k}\n# NEW CLASS — was completely missing from original notebook\n# This is what the unsupervised adaptation loop trains on\n# =============================================================================\nclass MemoryBank:\n    \"\"\"Full episode memory bank per environment — implements paper Eq.1.\n\n    After k instructions in environment E:\n        M_E = {X_1:k, O_1:k, A_1:k, P_1:k}\n    where:\n        X = instructions (language)\n        O = visual observations at each step\n        A = actions taken\n        P = trajectory paths\n\n    This is the data source for the UNSUPERVISED adaptation loop (Eq.3).\n    It's 'unsupervised' because we have no ground truth labels —\n    only what the agent itself experienced (which may have errors).\n    \"\"\"\n    def __init__(self, scene_id: str, max_episodes: int = 50):\n        self.scene_id = scene_id\n        self.max_episodes = max_episodes\n        # Each entry is one completed episode\n        self.episodes: List[Dict] = []  # X_i, O_i, A_i, P_i for each episode i\n\n    def add_episode(self,\n                    instruction_ids: torch.Tensor,   # X_i: tokenized instruction\n                    observations: List[np.ndarray],  # O_i: visual features at each step\n                    actions: List[int],              # A_i: action indices taken\n                    path: List[str]):                # P_i: viewpoint sequence\n        \"\"\"Store one completed episode in memory. Called after every navigation.\"\"\"\n        episode = {\n            'instruction_ids': instruction_ids.cpu(),   # move off GPU for storage\n            'observations': observations,               # list of np arrays\n            'actions': actions,                         # list of ints\n            'path': path,                               # list of viewpoint strings\n        }\n        self.episodes.append(episode)\n        # Cap memory size to avoid unbounded growth\n        if len(self.episodes) > self.max_episodes:\n            self.episodes.pop(0)  # evict oldest\n\n    def sample_batch(self, batch_size: int = 4) -> Optional[List[Dict]]:\n        \"\"\"Sample random episodes for unsupervised training.\n        Returns None if not enough episodes accumulated yet.\n        \"\"\"\n        if len(self.episodes) < batch_size:\n            return None  # not enough data yet — paper needs sufficient history\n        return random.sample(self.episodes, batch_size)\n\n    def __len__(self):\n        return len(self.episodes)\n\n\n@dataclass\nclass NavigationInstance:\n    \"\"\"Single instruction-path pair. Maps to paper's (X_i, P_i) tuple.\"\"\"\n    scene_id: str\n    instruction_id: str\n    instruction: str\n    path: List[str]\n    trajectory: List[Dict]\n\n    def instruction_tokens(self) -> List[str]:\n        return self.instruction.lower().split()\n\n\nclass R2RLikeDataset:\n    \"\"\"Synthetic R2R-like dataset.\n    Real paper: GSA-R2R with 150 scenes, 90K instruction-path pairs, 7 styles.\n    Here: 8 synthetic scenes, 4 instructions each = 32 total.\n    \"\"\"\n    def __init__(self, num_scenes: int = 8, instructions_per_scene: int = 6):\n        self.scenes = {}\n        self.instructions = []\n        self.vocab = self._build_vocab()\n        # CHANGE: scene_to_instructions groups by scene for sequential execution\n        self.scene_to_instructions: Dict[str, List[NavigationInstance]] = defaultdict(list)\n\n        print(f\"Creating dataset with {num_scenes} scenes, {instructions_per_scene} instructions each...\")\n\n        for scene_idx in range(num_scenes):\n            scene_id = f\"scene_{scene_idx:03d}\"\n            self.scenes[scene_id] = NavigationGraph(scene_id, num_nodes=20)\n            nodes = list(self.scenes[scene_id].nodes)\n\n            for instr_idx in range(instructions_per_scene):\n                start_idx = np.random.randint(0, len(nodes))\n                end_idx = np.random.randint(0, len(nodes))\n                try:\n                    path = nx.shortest_path(self.scenes[scene_id].graph,\n                                            nodes[start_idx], nodes[end_idx])\n                except nx.NetworkXNoPath:\n                    path = [nodes[start_idx]]\n\n                instruction = self._generate_instruction(path)\n                inst = NavigationInstance(\n                    scene_id=scene_id,\n                    instruction_id=f\"{scene_id}_instr_{instr_idx}\",\n                    instruction=instruction,\n                    path=path,\n                    trajectory=[{\n                        'viewpoint': vp,\n                        'position': self.scenes[scene_id].node_positions[vp],\n                        'feature': self.scenes[scene_id].node_features[vp],\n                    } for vp in path]\n                )\n                self.instructions.append(inst)\n                self.scene_to_instructions[scene_id].append(inst)  # CHANGE: index by scene\n\n        print(f\"Created {len(self.instructions)} instructions across {num_scenes} scenes\")\n        print(f\"Instructions per scene: {instructions_per_scene} (sequential execution for adaptation)\")\n\n    def _generate_instruction(self, path: List[str]) -> str:\n        templates = [\"go forward\", \"walk to the {} room\", \"navigate to {}\",\n                     \"move towards {}\", \"head in the direction of {}\"]\n        if len(path) <= 1:\n            return \"stop\"\n        template = np.random.choice(templates)\n        location = f\"vp_{np.random.randint(0, 5)}\"\n        return template.format(location) if \"{}\" in template else template\n\n    def _build_vocab(self) -> Dict:\n        words = ['go', 'walk', 'move', 'navigate', 'forward', 'backward',\n                 'left', 'right', 'turn', 'towards', 'to', 'the',\n                 'room', 'hallway', 'entrance', 'exit', 'direction',\n                 'vp', 'stop', 'continue', '<pad>', '<unk>']\n        return {word: idx for idx, word in enumerate(words)}\n\n    def get_scene(self, scene_id: str) -> NavigationGraph:\n        return self.scenes.get(scene_id)\n\n    def get_instructions_for_scene(self, scene_id: str) -> List[NavigationInstance]:\n        \"\"\"CHANGE: Returns instructions grouped by scene for sequential adaptation.\"\"\"\n        return self.scene_to_instructions.get(scene_id, [])\n\n    def get_scene_ids(self) -> List[str]:\n        return list(self.scenes.keys())\n\n    def split_scenes(self, train_ratio: float = 0.75):\n        \"\"\"CHANGE: Split by SCENE (not by instruction) — paper trains on some scenes, tests on others.\"\"\"\n        scene_ids = self.get_scene_ids()\n        random.shuffle(scene_ids)\n        split_idx = int(len(scene_ids) * train_ratio)\n        train_scenes = scene_ids[:split_idx]\n        val_scenes = scene_ids[split_idx:]\n        print(f\"Scene split: {len(train_scenes)} train scenes, {len(val_scenes)} val scenes\")\n        return train_scenes, val_scenes\n\n\nprint(\"Creating dataset...\")\ndataset = R2RLikeDataset(num_scenes=8, instructions_per_scene=6)\ntrain_scenes, val_scenes = dataset.split_scenes(train_ratio=0.75)","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-3","cell_type":"markdown","source":"## Section 3: Model Architecture\n\n**What changed vs. original:**\n- Added `mlm_head` Linear layer to fix the MLM bug (line 496 in original: logits had wrong shape)\n- `GraphMapEncoder` is unchanged — it was the best part of the original\n- `GSAVLNModel` now also returns hidden state for use in adaptation loss","metadata":{}},{"id":"cell-code-3","cell_type":"code","source":"class LanguageEncoder(nn.Module):\n    \"\"\"Simplified BERT-like encoder. Real paper uses BERT-base (12 layers, 768-dim).\"\"\"\n    def __init__(self, vocab_size: int, hidden_dim: int = 256, num_layers: int = 2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n        self.transformer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, nhead=4, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(self.transformer, num_layers=num_layers)\n        self.hidden_dim = hidden_dim\n\n    def forward(self, token_ids: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        embeds = self.embedding(token_ids)\n        attn_mask = (mask == 0)\n        return self.encoder(embeds, src_key_padding_mask=attn_mask)\n\n\nclass VisualEncoder(nn.Module):\n    \"\"\"Projects visual features into hidden space. Real paper uses ViT-B/16 (2048→768).\"\"\"\n    def __init__(self, input_dim: int = 256, hidden_dim: int = 256):\n        super().__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n    def forward(self, visual_features: torch.Tensor) -> torch.Tensor:\n        return self.projection(visual_features)\n\n\nclass GraphMapEncoder(nn.Module):\n    \"\"\"Graph attention encoder — current position queries over all graph nodes.\n    This is the core of GR-DUET's graph-based memory access.\n    Unchanged from original (it was correct).\n    \"\"\"\n    def __init__(self, hidden_dim: int = 256, num_heads: int = 4):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(\n            hidden_dim, num_heads=num_heads, batch_first=True, dropout=0.1\n        )\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(hidden_dim, 512), nn.ReLU(), nn.Linear(512, hidden_dim)\n        )\n\n    def forward(self, graph_embeds: torch.Tensor, current_pos: torch.Tensor) -> torch.Tensor:\n        query = current_pos.unsqueeze(1)  # [B, 1, D]\n        context, _ = self.attention(query, graph_embeds, graph_embeds)\n        context = context.squeeze(1)     # [B, D]\n        context = self.norm1(context + current_pos)\n        ffn_out = self.ffn(context)\n        return self.norm2(context + ffn_out)\n\n\nclass GSAVLNModel(nn.Module):\n    \"\"\"Full GSA-VLN navigation model.\n\n    CHANGE from original:\n    - Added `mlm_head`: nn.Linear(hidden_dim, vocab_size) to fix MLM bug\n      (original line 496 did: logits = embeds @ embeds.T → shape [B,L,L] not [B,L,vocab_size])\n    - Removed value_head: not in GR-DUET (paper uses IL not RL)\n    - forward() now also returns fused hidden state for adaptation loss\n    \"\"\"\n    def __init__(self, vocab_size: int, hidden_dim: int = 256):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n\n        self.language_encoder = LanguageEncoder(vocab_size, hidden_dim)\n        self.visual_encoder = VisualEncoder(256, hidden_dim)\n        self.graph_encoder = GraphMapEncoder(hidden_dim)\n\n        self.cross_modal_attention = nn.MultiheadAttention(\n            hidden_dim, num_heads=4, batch_first=True, dropout=0.1\n        )\n        self.action_decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n            nn.Dropout(0.1), nn.Linear(hidden_dim, hidden_dim)\n        )\n        # CHANGE: Added mlm_head to fix original bug.\n        # Original line 496: `logits = language_embeds @ language_embeds.T`\n        # That gives [B, L, L] — you can't predict vocab IDs from that shape.\n        # Fix: project [B, L, D] → [B, L, vocab_size]\n        self.mlm_head = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self,\n                instr_ids: torch.Tensor,\n                instr_mask: torch.Tensor,\n                visual_feature: torch.Tensor,\n                graph_embeds: torch.Tensor,\n                graph_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Returns:\n            action_logits: [B, N] — score for each graph node as next action\n            hidden_state:  [B, D] — fused representation (NEW: used in adaptation loss)\n        \"\"\"\n        language_embeds = self.language_encoder(instr_ids, instr_mask)  # [B, L, D]\n        language_summary = language_embeds.mean(dim=1)                  # [B, D]\n        visual_embeds = self.visual_encoder(visual_feature)             # [B, D]\n        graph_context = self.graph_encoder(graph_embeds, visual_embeds) # [B, D]\n\n        combined = language_summary + visual_embeds + graph_context     # [B, D]\n\n        fused, _ = self.cross_modal_attention(\n            query=language_embeds,\n            key=graph_embeds,\n            value=graph_embeds,\n            key_padding_mask=(graph_mask == 0)\n        )\n        fused_summary = fused.mean(dim=1)   # [B, D]\n\n        hidden_state = combined + fused_summary  # [B, D] — CHANGE: returned for adaptation\n        action_features = self.action_decoder(hidden_state)  # [B, D]\n\n        action_logits = torch.matmul(\n            action_features.unsqueeze(1),   # [B, 1, D]\n            graph_embeds.transpose(1, 2)    # [B, D, N]\n        ).squeeze(1)                        # [B, N]\n\n        return action_logits, hidden_state\n\n\nprint(\"Model architecture:\")\nprint(\"  LanguageEncoder: Transformer (2 layers, 4 heads, 256-dim)\")\nprint(\"  VisualEncoder:   MLP projection (256→256)\")\nprint(\"  GraphMapEncoder: Multi-head cross-attention over graph nodes\")\nprint(\"  GSAVLNModel:     Full fusion + action scoring\")\nprint(\"  mlm_head:        Linear(256, vocab_size) — FIXED from original\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-4","cell_type":"markdown","source":"## Section 4: Pretraining Tasks (Fixed)\n\n**What changed vs. original:**\n- `masked_language_modeling`: Fixed the bug on original line 496. Now uses `model.mlm_head` to get `[B, L, vocab_size]` logits\n- All other tasks unchanged — they were conceptually correct\n- Note: These 4 tasks are from DUET's pretraining (Chen et al. 2022), not introduced by GSA-VLN","metadata":{}},{"id":"cell-code-4","cell_type":"code","source":"class PretrainingTasks:\n    \"\"\"Multi-task pretraining objectives from DUET (Chen et al. 2022).\n    These are used to pretrain the general navigation model θ_0\n    before scene-specific adaptation (paper Eq.4).\n    \"\"\"\n\n    @staticmethod\n    def instruction_trajectory_matching(model, batch):\n        \"\"\"ITM: Does this instruction match this trajectory? Binary classification.\"\"\"\n        instr_ids = batch['instr_ids']\n        instr_mask = batch['instr_mask']\n        trajectory_features = batch['trajectory_features']\n        labels = batch['itm_labels']\n\n        language_embeds = model.language_encoder(instr_ids, instr_mask)\n        lang_summary = language_embeds.mean(dim=1)\n        traj_summary = trajectory_features.mean(dim=1)\n        match_score = torch.cosine_similarity(lang_summary, traj_summary)\n        return F.binary_cross_entropy_with_logits(match_score, labels.float())\n\n    @staticmethod\n    def masked_language_modeling(model, batch):\n        \"\"\"MLM: Predict masked tokens. Implements BERT-style MLM.\n\n        FIXED from original:\n        Original line 496: `logits = language_embeds @ language_embeds.transpose(-1, -2)`\n          → This gives shape [B, L, L] — similarity between positions, NOT vocab logits.\n          → Cannot compute cross_entropy against vocab token IDs with this.\n\n        Fix: use model.mlm_head (Linear: hidden_dim → vocab_size)\n          → Gives shape [B, L, vocab_size] — correct for predicting which token was masked.\n        \"\"\"\n        instr_ids = batch['instr_ids'].clone()\n        instr_mask = batch['instr_mask']\n\n        # Create masked input (15% masking, same as BERT)\n        masked_instr_ids = instr_ids.clone()\n        mlm_labels = torch.full_like(instr_ids, -100)  # -100 = ignore in cross_entropy\n        mask_prob = 0.15\n        for i in range(instr_ids.size(0)):\n            for j in range(instr_ids.size(1)):\n                if instr_mask[i, j] == 1 and np.random.rand() < mask_prob:\n                    mlm_labels[i, j] = instr_ids[i, j]  # save true label\n                    masked_instr_ids[i, j] = 0           # replace with mask token\n\n        # Encode masked instruction\n        language_embeds = model.language_encoder(masked_instr_ids, instr_mask)  # [B, L, D]\n\n        # FIXED: project to vocab space using mlm_head\n        logits = model.mlm_head(language_embeds)  # [B, L, vocab_size] ← CORRECT shape\n\n        # Loss only on masked positions (ignore_index=-100 skips unmasked)\n        loss = F.cross_entropy(\n            logits.view(-1, model.vocab_size),  # [B*L, vocab_size]\n            mlm_labels.view(-1),                # [B*L]\n            ignore_index=-100\n        )\n        return loss\n\n    @staticmethod\n    def visual_semantic_alignment(model, batch):\n        \"\"\"VSA: Align instruction with trajectory visual features.\"\"\"\n        instr_ids = batch['instr_ids']\n        instr_mask = batch['instr_mask']\n        trajectory_features = batch['trajectory_features']\n\n        language_embeds = model.language_encoder(instr_ids, instr_mask)\n        similarity = torch.bmm(language_embeds, trajectory_features.transpose(1, 2))\n        B, L, T = similarity.shape\n        # Each language token should align most with its corresponding step\n        # (simplified: token i aligns with min(i, T-1))\n        targets = torch.clamp(torch.arange(L, device=similarity.device), max=T-1)\n        targets = targets.unsqueeze(0).expand(B, -1).reshape(-1)\n        loss = F.cross_entropy(similarity.reshape(-1, T), targets)\n        return loss\n\n    @staticmethod\n    def graph_structure_learning(model, batch):\n        \"\"\"GSL: Predict adjacency between graph nodes.\"\"\"\n        graph_embeds = batch['graph_embeds']\n        connectivity = batch['connectivity']\n        similarity = torch.bmm(graph_embeds, graph_embeds.transpose(1, 2)) / np.sqrt(256)\n        return F.binary_cross_entropy_with_logits(similarity, connectivity.float())\n\n\ndef pretrain_one_epoch(model, optimizer, dataset, batch_size=4, num_steps=150):\n    model.train()\n    total_loss = 0\n    task_losses = {'itm': [], 'mlm': [], 'vsa': [], 'gsl': []}\n    task_weights = {'itm': 0.25, 'mlm': 0.25, 'vsa': 0.25, 'gsl': 0.25}\n\n    pbar = tqdm(range(num_steps), desc=\"Pretraining\")\n    for step in pbar:\n        optimizer.zero_grad()\n        batch = {\n            'instr_ids':           torch.randint(0, len(dataset.vocab), (batch_size, 20)).to(device),\n            'instr_mask':          torch.ones(batch_size, 20).to(device),\n            'trajectory_features': torch.randn(batch_size, 15, 256).to(device),\n            'graph_embeds':        torch.randn(batch_size, 10, 256).to(device),\n            'graph_mask':          torch.ones(batch_size, 10).to(device),\n            'itm_labels':          torch.randint(0, 2, (batch_size,)).float().to(device),\n            'connectivity':        torch.randint(0, 2, (batch_size, 10, 10)).to(device),\n        }\n        try:\n            losses = {\n                'itm': PretrainingTasks.instruction_trajectory_matching(model, batch),\n                'mlm': PretrainingTasks.masked_language_modeling(model, batch),\n                'vsa': PretrainingTasks.visual_semantic_alignment(model, batch),\n                'gsl': PretrainingTasks.graph_structure_learning(model, batch),\n            }\n            total_batch_loss = sum(w * losses[k] for k, w in task_weights.items())\n            for k in task_losses:\n                task_losses[k].append(losses[k].item())\n        except Exception as e:\n            continue\n\n        total_batch_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        total_loss += total_batch_loss.item()\n        pbar.set_postfix({'loss': f\"{total_loss/(step+1):.4f}\",\n                          'mlm': f\"{np.mean(task_losses['mlm'][-10:]):.4f}\" if task_losses['mlm'] else '?'})\n\n    return total_loss / num_steps\n\n\nprint(\"Pretraining tasks ready:\")\nprint(\"  ITM: Instruction-Trajectory Matching\")\nprint(\"  MLM: Masked Language Modeling (FIXED — now uses mlm_head for [B,L,vocab_size])\")\nprint(\"  VSA: Visual-Semantic Alignment\")\nprint(\"  GSL: Graph Structure Learning\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-5","cell_type":"markdown","source":"## Section 5: Unsupervised Adaptation Loop — Paper Eq.3\n\nThis is the **most important new section** — what was ~10% in the original.\n\n**Paper Eq.3:** `θ' = θ - α∇_θ L(M_E, θ)`\n\nAfter the agent executes each instruction, it:\n1. Stores the episode in `MemoryBank` (observations, actions, path)\n2. Samples a mini-batch from memory\n3. Computes an **unsupervised** loss (no ground truth labels needed)\n4. Updates model parameters `θ` with gradient descent\n\nThis happens **between episodes** in the same environment. It's 'unsupervised' because the memory contains the agent's own (possibly erroneous) trajectories — not ground truth.","metadata":{}},{"id":"cell-code-5","cell_type":"code","source":"# =============================================================================\n# UNSUPERVISED ADAPTATION LOSSES\n# Paper Eq.3: θ' = θ - α∇L(M_E, θ)\n# These losses operate on the memory bank WITHOUT ground truth labels.\n# =============================================================================\n\ndef compute_trajectory_consistency_loss(\n        model: nn.Module,\n        episode_batch: List[Dict],\n        dataset,\n        vocab: Dict) -> torch.Tensor:\n    \"\"\"Unsupervised Loss 1: Trajectory Consistency (Back-Translation style).\n\n    Paper Section 4 mentions Back-Translation as an optimization-based adaptation method.\n    Idea: if the agent's path is P, a good model should re-score those actions highly\n    when re-presented with the same instruction and observations.\n\n    This is unsupervised: we use the agent's OWN actions as pseudo-labels.\n    The model is rewarded for being self-consistent across visits to the same place.\n    \"\"\"\n    total_loss = torch.tensor(0.0, device=device, requires_grad=True)\n    count = 0\n\n    for ep in episode_batch:\n        instr_ids = ep['instruction_ids'].to(device)     # [1, L]\n        observations = ep['observations']                # list of np arrays\n        actions = ep['actions']                          # list of ints (pseudo-labels)\n\n        if len(observations) < 2 or len(actions) < 1:\n            continue\n\n        instr_mask = torch.ones_like(instr_ids, dtype=torch.float)\n\n        # Build graph from this episode's path (observed nodes only)\n        graph_embeds_list = [torch.from_numpy(obs).to(device) for obs in observations]\n        if len(graph_embeds_list) == 0:\n            continue\n        graph_embeds = torch.stack(graph_embeds_list).unsqueeze(0)  # [1, T, D]\n        graph_mask = torch.ones(1, len(graph_embeds_list), device=device)\n\n        # For each step in the remembered episode:\n        step_losses = []\n        for step_idx, (obs, action) in enumerate(zip(observations[:-1], actions)):\n            visual_feat = torch.from_numpy(obs).unsqueeze(0).to(device)  # [1, D]\n\n            action_logits, _ = model(instr_ids, instr_mask, visual_feat,\n                                     graph_embeds, graph_mask)\n\n            # Pseudo-label: the action the agent actually took (from memory)\n            # This is UNSUPERVISED: no ground truth, just self-consistency\n            if action < action_logits.size(1):\n                pseudo_label = torch.LongTensor([action]).to(device)\n                step_loss = F.cross_entropy(action_logits, pseudo_label)\n                step_losses.append(step_loss)\n\n        if step_losses:\n            ep_loss = torch.stack(step_losses).mean()\n            total_loss = total_loss + ep_loss\n            count += 1\n\n    return total_loss / max(count, 1)\n\n\ndef compute_observation_reconstruction_loss(\n        model: nn.Module,\n        episode_batch: List[Dict]) -> torch.Tensor:\n    \"\"\"Unsupervised Loss 2: Observation Reconstruction (Predictive Coding).\n\n    Idea: given the instruction and current observation, the model's hidden state\n    should be able to predict what it will see NEXT (next observation in the trajectory).\n\n    This is a form of predictive coding / self-supervised representation learning.\n    No ground truth labels needed — next observation is from memory.\n\n    Related to TENT (Wang et al. 2021) which uses entropy as an unsupervised signal.\n    \"\"\"\n    # Build a simple prediction head on-the-fly\n    # In a full implementation this would be a persistent learned head\n    pred_head = nn.Linear(256, 256).to(device)\n    nn.init.eye_(pred_head.weight)  # identity init\n\n    total_loss = torch.tensor(0.0, device=device, requires_grad=True)\n    count = 0\n\n    for ep in episode_batch:\n        observations = ep['observations']\n        if len(observations) < 2:\n            continue\n\n        for t in range(len(observations) - 1):\n            current_obs = torch.from_numpy(observations[t]).unsqueeze(0).to(device)    # [1, D]\n            next_obs    = torch.from_numpy(observations[t+1]).unsqueeze(0).to(device)  # [1, D]\n\n            # Predict next observation from current\n            predicted_next = pred_head(current_obs)  # [1, D]\n\n            # MSE between predicted and actual next observation\n            # Unsupervised: next_obs comes from memory, not from labels\n            recon_loss = F.mse_loss(predicted_next, next_obs.detach())\n            total_loss = total_loss + recon_loss\n            count += 1\n\n    return total_loss / max(count, 1)\n\n\ndef compute_entropy_minimization_loss(\n        model: nn.Module,\n        episode_batch: List[Dict]) -> torch.Tensor:\n    \"\"\"Unsupervised Loss 3: Entropy Minimization (TENT-style, Wang et al. 2021).\n\n    Paper Section 4.1 mentions TENT as a baseline adaptation method.\n    Idea: the model should be CONFIDENT about its action distribution.\n    High-entropy predictions = uncertain = model is confused about this scene.\n    Minimizing entropy = model becomes more decisive in this environment.\n\n    This is purely unsupervised: no labels needed, only the model's own output.\n    \"\"\"\n    total_loss = torch.tensor(0.0, device=device, requires_grad=True)\n    count = 0\n\n    for ep in episode_batch:\n        instr_ids = ep['instruction_ids'].to(device)\n        observations = ep['observations']\n        if len(observations) < 2:\n            continue\n\n        instr_mask = torch.ones_like(instr_ids, dtype=torch.float)\n\n        graph_embeds_list = [torch.from_numpy(obs).to(device) for obs in observations]\n        graph_embeds = torch.stack(graph_embeds_list).unsqueeze(0)  # [1, T, D]\n        graph_mask = torch.ones(1, len(graph_embeds_list), device=device)\n\n        for obs in observations[:-1]:\n            visual_feat = torch.from_numpy(obs).unsqueeze(0).to(device)\n            action_logits, _ = model(instr_ids, instr_mask, visual_feat,\n                                     graph_embeds, graph_mask)\n\n            # Entropy of action distribution: H = -Σ p log p\n            probs = F.softmax(action_logits, dim=-1)         # [1, N]\n            log_probs = F.log_softmax(action_logits, dim=-1) # [1, N]\n            entropy = -(probs * log_probs).sum(dim=-1).mean() # scalar\n\n            # Minimize entropy = maximize confidence\n            total_loss = total_loss + entropy\n            count += 1\n\n    return total_loss / max(count, 1)\n\n\n# =============================================================================\n# MAIN UNSUPERVISED ADAPTATION STEP — Paper Eq.3\n# θ' = θ - α∇_θ L(M_E, θ)\n# This is called AFTER each episode, using data from MemoryBank\n# =============================================================================\ndef unsupervised_adaptation_step(\n        model: nn.Module,\n        adaptation_optimizer: optim.Optimizer,\n        memory_bank: MemoryBank,\n        dataset,\n        batch_size: int = 4,\n        loss_weights: Dict[str, float] = None) -> Optional[float]:\n    \"\"\"Execute one step of unsupervised scene adaptation.\n\n    This implements Paper Eq.3: θ' = θ - α∇_θ L(M_E, θ)\n\n    Called AFTER each episode completes. Uses the accumulated MemoryBank\n    M_E to update model parameters WITHOUT any ground truth labels.\n\n    Returns:\n        adaptation loss value (float), or None if not enough memory yet\n    \"\"\"\n    if loss_weights is None:\n        loss_weights = {\n            'trajectory_consistency': 0.5,  # most important — self-consistency\n            'entropy_minimization':   0.3,  # TENT-style confidence\n            'observation_recon':      0.2,  # predictive coding\n        }\n\n    # Sample episodes from memory bank M_E\n    episode_batch = memory_bank.sample_batch(batch_size=batch_size)\n    if episode_batch is None:\n        return None  # not enough episodes accumulated yet\n\n    model.train()\n    adaptation_optimizer.zero_grad()\n\n    # Compute unsupervised losses on memory bank\n    losses = {}\n    try:\n        losses['trajectory_consistency'] = compute_trajectory_consistency_loss(\n            model, episode_batch, dataset, dataset.vocab)\n        losses['entropy_minimization'] = compute_entropy_minimization_loss(\n            model, episode_batch)\n        losses['observation_recon'] = compute_observation_reconstruction_loss(\n            model, episode_batch)\n    except Exception as e:\n        return None\n\n    # Total weighted loss: L(M_E, θ)\n    total_adaptation_loss = sum(\n        loss_weights.get(k, 0.0) * v for k, v in losses.items()\n        if isinstance(v, torch.Tensor)\n    )\n\n    if not isinstance(total_adaptation_loss, torch.Tensor):\n        return None\n\n    # Gradient step: θ ← θ - α∇L(M_E, θ)\n    total_adaptation_loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # smaller clip for adaptation\n    adaptation_optimizer.step()\n\n    return total_adaptation_loss.item()\n\n\nprint(\"Unsupervised adaptation loop implemented (Paper Eq.3):\")\nprint(\"  Loss 1: Trajectory Consistency — self-consistency on remembered actions\")\nprint(\"  Loss 2: Entropy Minimization   — TENT-style, maximize action confidence\")\nprint(\"  Loss 3: Observation Reconstruction — predict next obs from current\")\nprint(\"  Called AFTER each episode using MemoryBank M_E (no ground truth needed)\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-6","cell_type":"markdown","source":"## Section 6: Navigation Agent with Full GSA-VLN Loop\n\n**What changed vs. original:**\n\n| Location | Original Bug | Fix |\n|---|---|---|\n| Line 637 | `scenes_gmaps = {}` used trajectory[0] as start | Now uses `scene.get_fixed_start()` — stable anchor |\n| Line 665 | Fresh GraphMap when `use_gmap=False` | Persistent map always used; baseline uses separate agent |\n| Line 681 | Map updated but only within one call | `mark_episode_end()` called after each episode — map survives |\n| Line 728 | `loss.item()` killed gradient | Tensor losses collected, `.backward()` called properly |\n| NEW | No memory bank | `MemoryBank` populated after every episode |\n| NEW | No adaptation | `unsupervised_adaptation_step()` called after every episode |","metadata":{}},{"id":"cell-code-6","cell_type":"code","source":"class NavigationAgent:\n    \"\"\"Full GSA-VLN agent with persistent GraphMap + unsupervised adaptation.\n\n    Two optimizers:\n    1. supervised_optimizer:   for CIL loss during trajectory execution\n    2. adaptation_optimizer:   for unsupervised Eq.3 updates between episodes\n       (smaller lr — adaptation should be gentle, not overwrite general knowledge)\n    \"\"\"\n    def __init__(self, model: GSAVLNModel, dataset: R2RLikeDataset,\n                 use_adaptation: bool = True,\n                 adaptation_lr: float = 5e-5):\n        self.model = model\n        self.dataset = dataset\n        self.use_adaptation = use_adaptation\n\n        # CHANGE: One GraphMap per scene — PERSISTENT across ALL episodes in that scene\n        # Initialized with scene's fixed_start (not trajectory[0])\n        self.scenes_gmaps: Dict[str, GraphMap] = {}\n\n        # CHANGE: NEW — MemoryBank per scene (implements Eq.1: M_E)\n        self.memory_banks: Dict[str, MemoryBank] = {}\n\n        # Two separate optimizers\n        self.supervised_optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        # CHANGE: NEW — adaptation optimizer with SMALLER lr to avoid overwriting general knowledge\n        self.adaptation_optimizer = optim.Adam(model.parameters(), lr=adaptation_lr)\n\n        # Stats tracking\n        self.adaptation_losses: List[float] = []\n        self.supervised_losses: List[float] = []\n\n    def _init_scene(self, scene_id: str):\n        \"\"\"CHANGE: Initialize GraphMap and MemoryBank for a new scene.\n        Uses scene's fixed start node (not trajectory-dependent).\n        \"\"\"\n        if scene_id not in self.scenes_gmaps:\n            scene = self.dataset.get_scene(scene_id)\n            fixed_start = scene.get_fixed_start()  # stable anchor node\n            self.scenes_gmaps[scene_id] = GraphMap(fixed_start)\n            self.memory_banks[scene_id] = MemoryBank(scene_id)\n\n    def encode_instruction(self, instruction: str, max_len: int = 20) -> Tuple[torch.Tensor, torch.Tensor]:\n        tokens = instruction.lower().split()\n        token_ids = [self.dataset.vocab.get(t, self.dataset.vocab['<unk>'])\n                     for t in tokens[:max_len]]\n        token_ids += [self.dataset.vocab['<pad>']] * (max_len - len(token_ids))\n        ids_tensor  = torch.LongTensor(token_ids[:max_len]).unsqueeze(0).to(device)\n        mask_tensor = torch.zeros(1, max_len).to(device)\n        mask_tensor[0, :min(len(tokens), max_len)] = 1\n        return ids_tensor, mask_tensor\n\n    def execute_trajectory(self,\n                           scene_id: str,\n                           instruction: str,\n                           trajectory: List[Dict],\n                           train_supervised: bool = True,\n                           max_steps: int = 20) -> Dict:\n        \"\"\"Execute one navigation instruction, update GraphMap + MemoryBank.\n\n        CHANGES from original:\n        - GraphMap initialized from scene.get_fixed_start(), not trajectory[0]\n        - GraphMap.mark_episode_end() called at end — map persists\n        - Episode stored in MemoryBank after completion\n        - FIXED backprop bug: tensor losses collected, .backward() actually called\n        - After storing episode, calls unsupervised_adaptation_step() (Eq.3)\n        \"\"\"\n        self._init_scene(scene_id)  # creates GraphMap + MemoryBank if first visit\n\n        gmap   = self.scenes_gmaps[scene_id]   # PERSISTENT across episodes\n        memory = self.memory_banks[scene_id]   # PERSISTENT across episodes\n\n        instr_ids, instr_mask = self.encode_instruction(instruction)\n\n        current_vp       = trajectory[0]['viewpoint']\n        current_traj     = [current_vp]\n        episode_obs      = [trajectory[0]['feature']]  # O: observations\n        episode_actions  = []                           # A: actions taken\n\n        supervised_step_losses = []  # tensor losses for backprop\n        total_supervised_loss_val = 0.0\n        num_steps = 0\n\n        scene = self.dataset.get_scene(scene_id)\n\n        for step_idx, target_step in enumerate(\n                trajectory[1:min(max_steps + 1, len(trajectory))]):\n\n            # 1. Update GraphMap with current observation\n            neighbors = scene.get_neighbors(current_vp)\n            gmap.update_graph(\n                current_vp,\n                scene.node_positions[current_vp],\n                scene.get_feature(current_vp),\n                neighbors\n            )\n\n            # 2. Prepare inputs\n            visual_feat = torch.from_numpy(\n                scene.get_feature(current_vp)).unsqueeze(0).to(device)\n\n            graph_nodes = gmap.get_all_visited_nodes()\n            graph_embeds_tensors = [torch.zeros(256, device=device)]  # STOP token\n            for vp in graph_nodes:\n                graph_embeds_tensors.append(\n                    torch.from_numpy(gmap.get_node_embed(vp)).to(device))\n            graph_embeds = torch.stack(graph_embeds_tensors).unsqueeze(0)  # [1, N, D]\n            graph_mask   = torch.ones(1, graph_embeds.size(1)).to(device)\n\n            # 3. Forward pass\n            action_logits, _ = self.model(\n                instr_ids, instr_mask, visual_feat, graph_embeds, graph_mask)\n\n            # 4. Supervised loss (imitation learning on ground truth path)\n            target_vp = target_step['viewpoint']\n            if target_vp in graph_nodes:\n                target_action_idx = graph_nodes.index(target_vp) + 1  # +1 for STOP\n            else:\n                target_action_idx = 0  # default to STOP if not in graph\n\n            chosen_action = target_action_idx  # teacher forcing\n\n            if train_supervised and target_action_idx < action_logits.size(1):\n                target_tensor = torch.LongTensor([target_action_idx]).to(device)\n                # FIXED: keep as tensor, don't call .item() yet\n                step_loss = F.cross_entropy(action_logits, target_tensor)\n                supervised_step_losses.append(step_loss)        # tensor — gradient intact\n                total_supervised_loss_val += step_loss.item()   # float — for logging only\n                num_steps += 1\n\n            episode_actions.append(chosen_action)\n            current_vp = target_vp\n            current_traj.append(current_vp)\n            episode_obs.append(scene.get_feature(current_vp))\n\n        # -----------------------------------------------------------------------\n        # FIXED backprop: supervised loss (was broken in original)\n        # Original: loss.item() was called inside the loop → killed gradient\n        # Fix: collect tensor losses, backward ONCE at end\n        # -----------------------------------------------------------------------\n        if train_supervised and supervised_step_losses:\n            self.supervised_optimizer.zero_grad()\n            total_sup_loss = torch.stack(supervised_step_losses).mean()\n            total_sup_loss.backward()  # ← THIS was missing in original\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.supervised_optimizer.step()\n            self.supervised_losses.append(total_supervised_loss_val / max(num_steps, 1))\n\n        # -----------------------------------------------------------------------\n        # CHANGE: Mark episode end — GraphMap DOES NOT reset\n        # This is the key difference from standard VLN\n        # -----------------------------------------------------------------------\n        gmap.mark_episode_end()\n\n        # -----------------------------------------------------------------------\n        # CHANGE: Store episode in MemoryBank (Paper Eq.1)\n        # M_E grows with each completed episode\n        # -----------------------------------------------------------------------\n        memory.add_episode(\n            instruction_ids=instr_ids,\n            observations=episode_obs,\n            actions=episode_actions,\n            path=current_traj\n        )\n\n        # -----------------------------------------------------------------------\n        # CHANGE: Unsupervised adaptation after storing episode (Paper Eq.3)\n        # θ' = θ - α∇L(M_E, θ)\n        # This is the entire missing piece from original notebook\n        # -----------------------------------------------------------------------\n        adaptation_loss_val = None\n        if self.use_adaptation and len(memory) >= 4:  # need enough episodes first\n            adaptation_loss_val = unsupervised_adaptation_step(\n                self.model,\n                self.adaptation_optimizer,\n                memory,\n                self.dataset,\n                batch_size=min(4, len(memory))\n            )\n            if adaptation_loss_val is not None:\n                self.adaptation_losses.append(adaptation_loss_val)\n\n        final_vp      = trajectory[-1]['viewpoint']\n        reached_target = (current_vp == final_vp)\n\n        return {\n            'trajectory':      current_traj,\n            'success':         reached_target,\n            'steps':           len(current_traj) - 1,\n            'sup_loss':        total_supervised_loss_val / max(num_steps, 1),\n            'adapt_loss':      adaptation_loss_val,\n            'gmap_size':       gmap.node_count(),\n            'gmap_episodes':   gmap.episode_count,\n            'memory_size':     len(memory),\n        }\n\n\nprint(\"NavigationAgent ready with:\")\nprint(\"  - Persistent GraphMap (environment-scoped, not episode-scoped)\")\nprint(\"  - MemoryBank (Eq.1: stores O, X, A, P per scene)\")\nprint(\"  - Fixed supervised backprop (tensor losses, proper .backward())\")\nprint(\"  - Unsupervised adaptation after each episode (Eq.3)\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-7","cell_type":"markdown","source":"## Section 7: Training Pipeline — Sequential Per-Scene Execution\n\n**What changed vs. original:**\n- `finetune_one_epoch` now iterates **scenes first, then instructions within each scene**\n- This ensures GraphMap and MemoryBank accumulate properly before moving to the next scene\n- Tracks both supervised loss AND adaptation loss separately","metadata":{}},{"id":"cell-code-7","cell_type":"code","source":"def run_scene_adaptation(\n        agent: NavigationAgent,\n        dataset: R2RLikeDataset,\n        scene_ids: List[str],\n        train_supervised: bool = True) -> Dict:\n    \"\"\"Run the full GSA-VLN loop over a set of scenes.\n\n    CHANGE from original finetune_one_epoch:\n    - Outer loop: scenes (not random instructions)\n    - Inner loop: ALL instructions for that scene, IN ORDER\n    - GraphMap grows across the inner loop → later instructions benefit from earlier ones\n    - Memory bank accumulates → unsupervised adaptation gets richer data over time\n\n    This matches paper Figure 1: agent executes many instructions in ONE scene,\n    becoming increasingly familiar with it over time.\n    \"\"\"\n    agent.model.train()\n\n    all_results = []\n    scene_summaries = []\n\n    for scene_id in tqdm(scene_ids, desc=\"Scenes\"):\n        instructions = dataset.get_instructions_for_scene(scene_id)\n        if not instructions:\n            continue\n\n        scene_success = []\n        scene_gmap_sizes = []\n        scene_adapt_losses = []\n        scene_sup_losses = []\n\n        # Execute ALL instructions for this scene SEQUENTIALLY\n        # GraphMap and MemoryBank accumulate across this inner loop\n        for i, inst in enumerate(instructions):\n            result = agent.execute_trajectory(\n                inst.scene_id,\n                inst.instruction,\n                inst.trajectory,\n                train_supervised=train_supervised,\n                max_steps=15\n            )\n            all_results.append(result)\n            scene_success.append(result['success'])\n            scene_gmap_sizes.append(result['gmap_size'])\n            scene_sup_losses.append(result['sup_loss'])\n            if result['adapt_loss'] is not None:\n                scene_adapt_losses.append(result['adapt_loss'])\n\n        summary = {\n            'scene_id':           scene_id,\n            'num_instructions':   len(instructions),\n            'success_rate':       np.mean(scene_success),\n            'final_gmap_size':    scene_gmap_sizes[-1] if scene_gmap_sizes else 0,\n            'avg_sup_loss':       np.mean(scene_sup_losses) if scene_sup_losses else 0,\n            'avg_adapt_loss':     np.mean(scene_adapt_losses) if scene_adapt_losses else 0,\n            'num_adapt_steps':    len(scene_adapt_losses),\n            # Track improvement: first half vs second half of instructions\n            'early_success':      np.mean(scene_success[:len(scene_success)//2]) if scene_success else 0,\n            'late_success':       np.mean(scene_success[len(scene_success)//2:]) if scene_success else 0,\n        }\n        scene_summaries.append(summary)\n\n    return {\n        'all_results':     all_results,\n        'scene_summaries': scene_summaries,\n        'overall_success': np.mean([r['success'] for r in all_results]),\n        'avg_sup_loss':    np.mean([r['sup_loss'] for r in all_results]),\n        'avg_gmap_size':   np.mean([r['gmap_size'] for r in all_results]),\n    }\n\n\nprint(\"Training loop configured for sequential per-scene execution\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-8","cell_type":"markdown","source":"## Section 8: Run the Full Pipeline","metadata":{}},{"id":"cell-code-8","cell_type":"code","source":"print(\"=\" * 70)\nprint(\"GSA-VLN FULL PIPELINE WITH CROSS-EPISODE ADAPTATION\")\nprint(\"=\" * 70)\n\nvocab_size = len(dataset.vocab)\nmodel = GSAVLNModel(vocab_size=vocab_size, hidden_dim=256).to(device)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# ---- PHASE 1: PRETRAINING (Paper Eq.4) ----\nprint(\"\\n[PHASE 1] PRETRAINING — Learning general navigation (θ_0)\")\nprint(\"-\" * 70)\noptimizer_pretrain = optim.Adam(model.parameters(), lr=1e-3)\npretrain_loss = pretrain_one_epoch(\n    model, optimizer_pretrain, dataset, batch_size=4, num_steps=150)\nprint(f\"Pretraining done. Loss: {pretrain_loss:.4f}\")\n\n# Save pretrained weights — adaptation starts from this θ_0\npretrained_state = copy.deepcopy(model.state_dict())\nprint(\"Pretrained weights saved as θ_0\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-code-9","cell_type":"code","source":"# ---- PHASE 2: BASELINE (no adaptation) ----\nprint(\"\\n[PHASE 2a] BASELINE — Standard VLN, no scene adaptation\")\nprint(\"-\" * 70)\nprint(\"Agent: NO unsupervised adaptation, NO persistent GraphMap\")\n\n# Reset model to pretrained weights\nbaseline_model = GSAVLNModel(vocab_size=vocab_size, hidden_dim=256).to(device)\nbaseline_model.load_state_dict(copy.deepcopy(pretrained_state))\n\nbaseline_agent = NavigationAgent(\n    baseline_model, dataset,\n    use_adaptation=False  # ← No unsupervised adaptation\n)\n# Disable GraphMap persistence by using a fresh agent per scene\n# (scenes_gmaps stays empty — each call creates a fresh map and discards it)\n\nbaseline_results = run_scene_adaptation(\n    baseline_agent, dataset, val_scenes, train_supervised=True)\n\nprint(f\"\\nBaseline Results:\")\nprint(f\"  Overall Success Rate: {baseline_results['overall_success']*100:.1f}%\")\nprint(f\"  Avg Supervised Loss:  {baseline_results['avg_sup_loss']:.4f}\")\nprint(f\"  Avg GraphMap Size:    {baseline_results['avg_gmap_size']:.1f} nodes\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-code-10","cell_type":"code","source":"# ---- PHASE 2b: FULL GSA-VLN (with cross-episode adaptation) ----\nprint(\"\\n[PHASE 2b] GSA-VLN — With persistent GraphMap + unsupervised adaptation\")\nprint(\"-\" * 70)\nprint(\"Agent: Persistent GraphMap + MemoryBank + Eq.3 parameter updates\")\n\n# Reset model to same pretrained weights for fair comparison\ngsa_model = GSAVLNModel(vocab_size=vocab_size, hidden_dim=256).to(device)\ngsa_model.load_state_dict(copy.deepcopy(pretrained_state))\n\ngsa_agent = NavigationAgent(\n    gsa_model, dataset,\n    use_adaptation=True,   # ← Unsupervised adaptation ENABLED\n    adaptation_lr=5e-5     # ← Smaller lr than supervised to preserve general knowledge\n)\n\ngsa_results = run_scene_adaptation(\n    gsa_agent, dataset, val_scenes, train_supervised=True)\n\nprint(f\"\\nGSA-VLN Results:\")\nprint(f\"  Overall Success Rate:    {gsa_results['overall_success']*100:.1f}%\")\nprint(f\"  Avg Supervised Loss:     {gsa_results['avg_sup_loss']:.4f}\")\nprint(f\"  Avg GraphMap Size:       {gsa_results['avg_gmap_size']:.1f} nodes\")\nprint(f\"  Adaptation steps taken:  {len(gsa_agent.adaptation_losses)}\")\nif gsa_agent.adaptation_losses:\n    print(f\"  Avg Adaptation Loss:     {np.mean(gsa_agent.adaptation_losses):.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-9","cell_type":"markdown","source":"## Section 9: Results & Analysis","metadata":{}},{"id":"cell-code-11","cell_type":"code","source":"print(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS: Impact of Cross-Episode Adaptation\")\nprint(\"=\" * 70)\n\nsr_baseline = baseline_results['overall_success'] * 100\nsr_gsa      = gsa_results['overall_success'] * 100\nimprovement = sr_gsa - sr_baseline\n\nprint(f\"\\n{'Method':<35} {'Success Rate':>12} {'GraphMap Nodes':>15}\")\nprint(\"-\" * 65)\nprint(f\"{'Baseline (no adaptation)':<35} {sr_baseline:>11.1f}% {baseline_results['avg_gmap_size']:>15.1f}\")\nprint(f\"{'GSA-VLN (full adaptation)':<35} {sr_gsa:>11.1f}% {gsa_results['avg_gmap_size']:>15.1f}\")\nprint(\"-\" * 65)\ndelta_sign = '+' if improvement >= 0 else ''\nprint(f\"{'Improvement':<35} {delta_sign}{improvement:>10.1f}%\")\n\n# Per-scene breakdown\nprint(\"\\nPer-scene breakdown (GSA-VLN):\")\nprint(f\"{'Scene':<12} {'#Instrs':>8} {'Success':>8} {'GraphMap':>10} {'Adapt Steps':>12} {'Early→Late SR':>14}\")\nprint(\"-\" * 70)\nfor s in gsa_results['scene_summaries']:\n    early_late = f\"{s['early_success']*100:.0f}%→{s['late_success']*100:.0f}%\"\n    print(f\"{s['scene_id']:<12} {s['num_instructions']:>8} \"\n          f\"{s['success_rate']*100:>7.1f}% {s['final_gmap_size']:>10} \"\n          f\"{s['num_adapt_steps']:>12} {early_late:>14}\")\n\nprint(\"\\nNote: 'Early→Late SR' shows success rate in first half vs second half\")\nprint(\"of instructions. If adaptation works, Late SR > Early SR.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-code-12","cell_type":"code","source":"# ---- Visualization ----\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('GSA-VLN: Cross-Episode Adaptation Analysis', fontsize=14, fontweight='bold')\n\n# Plot 1: Success rate comparison\nax = axes[0, 0]\nmethods = ['Baseline\\n(no adaptation)', 'GSA-VLN\\n(full adaptation)']\nrates   = [sr_baseline, sr_gsa]\ncolors  = ['#e74c3c', '#2ecc71']\nbars = ax.bar(methods, rates, color=colors, width=0.4, edgecolor='black', linewidth=0.8)\nfor bar, rate in zip(bars, rates):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n            f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\nax.set_ylabel('Success Rate (%)')\nax.set_title('Overall Success Rate')\nax.set_ylim(0, max(rates) * 1.3 + 10)\nax.grid(axis='y', alpha=0.3)\n\n# Plot 2: GraphMap growth over instructions (for GSA-VLN)\nax = axes[0, 1]\ngmap_sizes = [r['gmap_size'] for r in gsa_results['all_results']]\nax.plot(range(len(gmap_sizes)), gmap_sizes, color='#3498db', linewidth=2, marker='o', markersize=3)\nax.set_xlabel('Instruction Number (across all scenes)')\nax.set_ylabel('GraphMap Nodes')\nax.set_title('GraphMap Growth Across Episodes')\nax.grid(alpha=0.3)\nax.fill_between(range(len(gmap_sizes)), gmap_sizes, alpha=0.1, color='#3498db')\n\n# Plot 3: Adaptation loss over time\nax = axes[1, 0]\nif gsa_agent.adaptation_losses:\n    adapt_losses = gsa_agent.adaptation_losses\n    ax.plot(adapt_losses, color='#9b59b6', linewidth=1.5, alpha=0.7, label='per step')\n    window = min(5, len(adapt_losses))\n    if len(adapt_losses) >= window:\n        smoothed = np.convolve(adapt_losses, np.ones(window)/window, mode='valid')\n        ax.plot(range(window-1, len(adapt_losses)), smoothed,\n                color='#6c3483', linewidth=2.5, label=f'smoothed (w={window})')\n    ax.set_xlabel('Adaptation Step')\n    ax.set_ylabel('Adaptation Loss (Eq.3)')\n    ax.set_title('Unsupervised Adaptation Loss (θ updated via Eq.3)')\n    ax.legend()\n    ax.grid(alpha=0.3)\nelse:\n    ax.text(0.5, 0.5, 'No adaptation steps\\n(need ≥4 episodes in memory)',\n            ha='center', va='center', transform=ax.transAxes, fontsize=11)\n    ax.set_title('Unsupervised Adaptation Loss')\n\n# Plot 4: Early vs Late success per scene (shows adaptation benefit)\nax = axes[1, 1]\nif gsa_results['scene_summaries']:\n    summaries = gsa_results['scene_summaries']\n    scene_labels = [s['scene_id'].replace('scene_', 'S') for s in summaries]\n    early_srs = [s['early_success'] * 100 for s in summaries]\n    late_srs  = [s['late_success']  * 100 for s in summaries]\n    x = np.arange(len(scene_labels))\n    w = 0.35\n    ax.bar(x - w/2, early_srs, w, label='Early instrs', color='#e67e22', alpha=0.8)\n    ax.bar(x + w/2, late_srs,  w, label='Late instrs',  color='#27ae60', alpha=0.8)\n    ax.set_xticks(x)\n    ax.set_xticklabels(scene_labels)\n    ax.set_ylabel('Success Rate (%)')\n    ax.set_title('Early vs Late Success per Scene\\n(Late > Early = adaptation working)')\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('gsa_vln_adaptation_results.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Figure saved.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cell-md-10","cell_type":"markdown","source":"## Section 10: Summary — What Changed and Why","metadata":{}},{"id":"cell-code-13","cell_type":"code","source":"print(\"\"\"\n╔══════════════════════════════════════════════════════════════════════════╗\n║         GSA-VLN IMPROVED: CHANGE LOG vs. ORIGINAL NOTEBOOK             ║\n╚══════════════════════════════════════════════════════════════════════════╝\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 1 — GraphMap: scene-level persistence (was episode-level)\n══════════════════════════════════════════════════════════════════════════\nORIGINAL (line 662-665):\n    if scene_id not in self.scenes_gmaps:\n        self.scenes_gmaps[scene_id] = GraphMap(trajectory[0]['viewpoint'])\n    gmap = self.scenes_gmaps[scene_id] if use_gmap else GraphMap(trajectory[0])\n\nPROBLEMS:\n  a) trajectory[0]['viewpoint'] changes each episode → unstable anchor\n  b) use_gmap=False creates a fresh map → map not used as ablation variable\n\nFIX:\n    fixed_start = scene.get_fixed_start()  # always 'vp_0' for this scene\n    self.scenes_gmaps[scene_id] = GraphMap(fixed_start)\n    gmap = self.scenes_gmaps[scene_id]  # ALWAYS use persistent map\n    gmap.update_graph(...)  # called every step\n    gmap.mark_episode_end() # called after each episode — MAP IS NOT RESET\n\nPAPER REFERENCE: GR-DUET Section 4.1 — \"The graph is retained across episodes\"\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 2 — MemoryBank: implements Paper Eq.1 (was entirely missing)\n══════════════════════════════════════════════════════════════════════════\nNEW class: MemoryBank\n    M_E = {X_1:k, O_1:k, A_1:k, P_1:k}  ← paper Eq.1\n    after each episode: memory.add_episode(instr_ids, observations, actions, path)\n\nWHY: Without M_E, there's nothing to run unsupervised adaptation ON.\n     The memory bank is what makes the adaptation 'unsupervised' —\n     we train on what the agent itself experienced, not on labels.\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 3 — Unsupervised Adaptation Loop: Paper Eq.3 (was ~10% before)\n══════════════════════════════════════════════════════════════════════════\nNEW function: unsupervised_adaptation_step(model, optimizer, memory_bank)\n\nImplements: θ' = θ - α∇_θ L(M_E, θ)  ← paper Eq.3\n\nThree unsupervised losses:\n  L1 — Trajectory Consistency: re-score remembered actions as pseudo-labels\n  L2 — Entropy Minimization:   TENT-style, reduce action distribution entropy\n  L3 — Observation Reconstruction: predict next obs from current (predictive coding)\n\nCalled: AFTER every episode (not during), using sampled batch from M_E\nOptimizer: separate adaptation_optimizer with SMALLER lr (5e-5 vs 1e-4)\n   → gentle adaptation, doesn't overwrite general knowledge from pretraining\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 4 — Fixed MLM Bug (original line 496)\n══════════════════════════════════════════════════════════════════════════\nORIGINAL (line 496):\n    logits = language_embeds @ language_embeds.transpose(-1, -2)  # [B, L, L]\n    → Shape [B, L, L]: similarity between token POSITIONS, not vocab logits\n    → Cannot predict which vocab token was masked from this\n\nFIX:\n    Added: self.mlm_head = nn.Linear(hidden_dim, vocab_size)  in GSAVLNModel\n    Used:  logits = model.mlm_head(language_embeds)  # [B, L, vocab_size] ✓\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 5 — Fixed Backprop Bug (original lines 728-729, 776)\n══════════════════════════════════════════════════════════════════════════\nORIGINAL:\n    loss = F.cross_entropy(action_logits, target_tensor)\n    total_loss += loss.item()  ← .item() kills the computation graph!\n    ...later...\n    total_loss += result['loss']  ← this is a float, can't .backward() on it\n    → Model weights NEVER UPDATED during fine-tuning in original\n\nFIX:\n    supervised_step_losses.append(step_loss)        # keep as tensor\n    total_supervised_loss_val += step_loss.item()   # float for logging only\n    ...after episode...\n    total_sup_loss = torch.stack(supervised_step_losses).mean()\n    total_sup_loss.backward()   ← gradients actually flow now\n    self.supervised_optimizer.step()\n\n══════════════════════════════════════════════════════════════════════════\nCHANGE 6 — Sequential scene execution (was random sampling)\n══════════════════════════════════════════════════════════════════════════\nORIGINAL: randomly sampled any instruction from any scene each step\n    → GraphMap never accumulates properly within one scene\n\nFIX: run_scene_adaptation() iterates scenes → then all instructions per scene\n    for scene_id in scene_ids:           # outer: one scene at a time\n        for inst in instructions[scene]: # inner: all instructions in order\n            execute_trajectory(...)      # GraphMap + MemoryBank grow here\n\n══════════════════════════════════════════════════════════════════════════\nWHAT STILL DIFFERS FROM REAL PAPER:\n══════════════════════════════════════════════════════════════════════════\nReal GR-DUET:                   This notebook:\n─────────────────────────       ─────────────────────────────────────\nCLIP/ViT-B/16 features (2048)   Random 256-dim vectors\nBERT-base (12 layers, 768d)     Lightweight transformer (2 layers, 256d)\nMatterport3D simulator          Synthetic NetworkX graphs\n150 scenes, 90K instructions    8 scenes, 48 instructions\nDual-scale graph transformer    Single-scale graph attention\n7 instruction style types       1 style (random templates)\n\nCore algorithm: correctly implemented\n  ✅ Eq.1 — MemoryBank M_E\n  ✅ Eq.2 — H_0 from GraphMap (persistent cross-episode)\n  ✅ Eq.3 — θ' = θ - α∇L(M_E, θ) (unsupervised adaptation)\n  ✅ Eq.4 — Pretraining θ_0 before adaptation\n\"\"\")","metadata":{},"outputs":[],"execution_count":null}]}